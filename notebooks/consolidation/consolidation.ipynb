{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from deepdiff import DeepDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/akumar/nse/neural_control')\n",
    "from loaders import load_sabes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_decoding_files = glob.glob('/mnt/Secondary/data/peanut_decoding01/peanut_decoding01_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in peanut_decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "peanut_decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(peanut_decoding_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_files = glob.glob('/mnt/Secondary/data/sabes_decoding01/sabes_decoding01_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in sabes_decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_decoding_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conslidating 4/27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indyparametric_files = glob.glob('/mnt/Secondary/data/indy_dimreduc_parametric/indy_dimreduc_parametric_*.dat')\n",
    "argfiles = glob.glob('/mnt/Secondary/data/indy_dimreduc_parametric/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_parametric_dimreduc_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(dimreduc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimreduc_files = glob.glob('/mnt/Secondary/data/indy_trialized_dimreduc/indy_trialized_dimreduc_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in dimreduc_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indy_trialized_dimreduc_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(dimreduc_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_files = glob.glob('/mnt/Secondary/data/peanut_decoding_fcca/peanut_decoding_fcca_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for file in decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_fcca_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(decoding_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_files = glob.glob('/mnt/Secondary/data/indydimreducparam_decoding/indydimreducparam_decoding_*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "\n",
    "result_list = []\n",
    "for file in decoding_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indydimreducparam_decoding/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_args = []\n",
    "for argfile in argfiles:\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    \n",
    "    # Track down the arg file associated with the dimreduc result\n",
    "    dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "    argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "    \n",
    "    with open(argfile, 'rb') as f:\n",
    "        dimreduc_args = pickle.load(f)\n",
    "    \n",
    "    loader_args.append(dimreduc_args['loader_args'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_loader_args = []\n",
    "for loader_arg in loader_args:\n",
    "    if not unique_loader_args:\n",
    "        unique_loader_args.append(loader_arg)\n",
    "    else:\n",
    "        diffs = []\n",
    "        for ula in unique_loader_args:\n",
    "            diffs.append(DeepDiff(ula, loader_arg))\n",
    "        \n",
    "        diffs = np.array([len(d) for d in diffs])\n",
    "        if np.all(diffs > 0):\n",
    "            unique_loader_args.append(loader_arg)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separately agregarate and save for each loader arg\n",
    "for i, ula in enumerate(unique_loader_args):\n",
    "    results = []\n",
    "    for loader_arg, argfile in zip(loader_args, argfiles):\n",
    "        if len(DeepDiff(loader_arg, ula)) == 0:\n",
    "            # Open up the results file\n",
    "            with open(argfile, 'rb') as f:\n",
    "                args = pickle.load(f)\n",
    "\n",
    "            # Open up the dimreduc args save the args with the results\n",
    "            # Track down the arg file associated with the dimreduc result\n",
    "            dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "            dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "            argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "            \n",
    "            with open(argfile, 'rb') as f:\n",
    "                dimreduc_args = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "            results_file = args['results_file']\n",
    "            with open(results_file, 'rb') as f:\n",
    "                result = pickle.load(f)\n",
    "                for result_ in result:\n",
    "                    for k, v in args.items():\n",
    "                        result_[k] = v\n",
    "                    for k, v in dimreduc_args.items():\n",
    "                        result_[k] = v\n",
    "\n",
    "            results.extend(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    with open('/home/akumar/nse/neural_control/data/indy_parametric_decoding/df%d.dat' % i, 'wb') as f:\n",
    "        f.write(pickle.dumps(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim                                                               10\n",
       "fold_idx                                                           0\n",
       "train_idxs         [11706, 11707, 11708, 11709, 11710, 11711, 117...\n",
       "test_idxs          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "dimreduc_method                                                  DCA\n",
       "dimreduc_args                                  {'T': 3, 'n_init': 5}\n",
       "coef               [[0.043348193288712804, 0.06172647904631565, -...\n",
       "score                                                      68.058941\n",
       "decoder                                                           lr\n",
       "decoder_args       {'trainlag': 4, 'testlag': 4, 'decoding_window...\n",
       "decoder_obj                         LinearRegression(normalize=True)\n",
       "r2                 [0.14425542097701782, 0.2055141988659695, 0.02...\n",
       "data_file             /mnt/Secondary/data/sabes/indy_20160630_01.mat\n",
       "loader                                                         sabes\n",
       "loader_args        {'bin_width': 25, 'filter_fn': 'window', 'filt...\n",
       "task_args          {'dim_vals': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1...\n",
       "data_path                                  /mnt/Secondary/data/sabes\n",
       "results_file       /mnt/Secondary/data/indy_dimreduc_parametric/i...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/indytrialized_decoding/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "not_done = []\n",
    "for argfile in argfiles:\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    # Open up the dimreduc args save the args with the results\n",
    "    # Track down the arg file associated with the dimreduc result\n",
    "    dimreducpath = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreducno = args['task_args']['dimreduc_file'].split('.dat')[0].split('_')[-1]\n",
    "    argfile = '%s/arg%s.dat' % (dimreducpath, dimreducno)    \n",
    "    \n",
    "    with open(argfile, 'rb') as f:\n",
    "        dimreduc_args = pickle.load(f)\n",
    "\n",
    "    results_file = args['results_file']\n",
    "    try:\n",
    "        with open(results_file, 'rb') as f:\n",
    "            result = pickle.load(f)\n",
    "            for result_ in result:\n",
    "                for k, v in args.items():\n",
    "                    result_[k] = v\n",
    "                for k, v in dimreduc_args.items():\n",
    "                    result_[k] = v\n",
    "    except FileNotFoundError:\n",
    "        not_done.append(results_file)\n",
    "    result_list.extend(result)\n",
    "decoding_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [int(n.split('.dat')[0].split('_')[-1]) for n in not_done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files that did not complete here are results of T being too long for trialized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,  189,  193,  197,  205,  209,  213,  217,  221,  229,  233,\n",
       "        237,  433,  437,  441,  449,  453,  457,  461,  465,  469,  637,\n",
       "        909, 1245, 1249, 1253, 1397, 1441, 1445, 1449, 1457, 1461, 1465,\n",
       "       1469, 1473, 1477])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/indytrialized_decoding_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(decoding_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter loco UoI data files and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/Secondary/data/sabes'    \n",
    " \n",
    "# These are the data files that contain both M1 and S1 recordings.\n",
    "data_files = glob.glob('%s/loco*' % data_path)\n",
    "data_files = [data_files[0], data_files[5]]\n",
    "loader_args = {'bin_width':50, 'filter_fn':'none', 'filter_kwargs':{}, 'boxcox':0.5, 'spike_threshold':100, 'region':'S1'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.20s/it]\n"
     ]
    }
   ],
   "source": [
    "d1 = load_sabes(data_files[0], **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.91s/it]\n"
     ]
    }
   ],
   "source": [
    "d2 = load_sabes(data_files[1], **loader_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'loco_20170210_03.mat'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files[0].split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/Secondary/data/sabes/preprocessed/%s' % data_files[0].split('/')[-1], 'wb') as f:\n",
    "    f.write(pickle.dumps(d1))\n",
    "    f.write(pickle.dumps((data_files[0], loader_args)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/Secondary/data/sabes/preprocessed/%s' % data_files[1].split('/')[-1], 'wb') as f:\n",
    "    f.write(pickle.dumps(d2))\n",
    "    f.write(pickle.dumps((data_files[1], loader_args)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidating CV Dimreudc with longer T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/cv_dimreduc2/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    result_list.extend(result)\n",
    "dimreduc_df = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_df2.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 10, 'n_init': 5}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimreduc_df.iloc[0]['dimreduc_args']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV_dimreduc aross both subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/cv_dimreduc_both/arg*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = []\n",
    "for f_ in argfiles:\n",
    "    with open(f_, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            result_[k] = v\n",
    "    \n",
    "    rl.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_both.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(rl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indy signfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "argfiles = glob.glob('/mnt/Secondary/data/sabes_signfix_dw5/arg*.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(argfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for argfile in argfiles:\n",
    "\n",
    "    # Open up the arg files\n",
    "    with open(argfile, 'rb') as f:\n",
    "        args = pickle.load(f)\n",
    "\n",
    "    rfile = args['results_file']\n",
    "    with open(rfile, 'rb') as f:\n",
    "        result = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    # Need to grab some information from the dimreduc arg file\n",
    "    dimreduc_path = '/'.join(args['task_args']['dimreduc_file'].split('/')[:-1])\n",
    "    dimreduc_no = args['task_args']['dimreduc_file'].split('_')[-1].split('.dat')[0]\n",
    "    dimreduc_argfile = '%s/arg%s.dat' % (dimreduc_path, dimreduc_no)\n",
    "\n",
    "    with open(dimreduc_argfile, 'rb') as f:\n",
    "        dr_args = pickle.load(f)\n",
    "\n",
    "    for result_ in result:\n",
    "        for k, v in dr_args.items():\n",
    "            if type(k) == dict:\n",
    "                for k_, v_ in k.items():\n",
    "                    result_[k_] = v_\n",
    "            else:\n",
    "                result_[k] = v\n",
    "\n",
    "    result_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8400"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dim', 'fold_idx', 'train_idxs', 'test_idxs', 'dimreduc_method', 'dimreduc_args', 'coef', 'score', 'decoder', 'decoder_args', 'decoder_obj', 'r2', 'task_args', 'data_file', 'data_path', 'loader', 'loader_args', 'results_file'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_sf.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(result_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import apply_df_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = np.unique(rl['data_file'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/mnt/Secondary/data/sabes/indy_20160426_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160622_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160624_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160627_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160630_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160915_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160921_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160930_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20160930_05.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161005_06.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161006_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161007_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161011_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161013_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161014_04.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161017_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161024_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161025_04.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161026_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161027_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161206_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161207_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161212_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20161220_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170123_02.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170124_01.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170127_03.mat',\n",
       "       '/mnt/Secondary/data/sabes/indy_20170131_02.mat'], dtype=object)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 18)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apply_df_filters(rl, data_file=data_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1200, 18)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1200"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 * 5 * 4 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20400 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c52494c424e88c3f855a8aeb34b231af4706f7aa247f66fb47c890a5ab8814ab"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
