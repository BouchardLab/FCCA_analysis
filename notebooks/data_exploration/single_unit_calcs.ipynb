{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import time\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/akumar/nse/neural_control')\n",
    "from utils import apply_df_filters, calc_loadings, calc_cascaded_loadings\n",
    "from loaders import load_sabes, load_peanut, load_cv\n",
    "from decoders import lr_decoder, lr_encoder\n",
    "from subspaces import SubspaceIdentification, IteratedStableEstimator, estimate_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dca.cov_util import calc_cross_cov_mats_from_data, calc_pi_from_cross_cov_mats\n",
    "from dca_research.kca import calc_mmse_from_cross_cov_mats\n",
    "from dca_research.lqg import build_loss as build_lqg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_path = '/mnt/sdb1/nc_data/'\n",
    "data_path = '/home/akumar/nse/neural_control/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/sabes_decoding_df.dat' % data_path, 'rb') as f:\n",
    "    sabes_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/peanut_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    peanut_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/cv_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    cv_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/loco_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    loco_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single unit calculations - no trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim                                                               10\n",
       "fold_idx                                                           0\n",
       "train_idxs         [1983, 1984, 1985, 1986, 1987, 1988, 1989, 199...\n",
       "test_idxs          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "dimreduc_method                                                  KCA\n",
       "dimreduc_args        {'T': 3, 'causal_weights': (1, 1), 'n_init': 5}\n",
       "coef               [[0.04434436239872305, -0.1047764350198416, -0...\n",
       "score                          tensor(141.1882, dtype=torch.float64)\n",
       "bin_width                                                         50\n",
       "filter_fn                                                       none\n",
       "filter_kwargs                                                     {}\n",
       "boxcox                                                           0.5\n",
       "spike_threshold                                                  100\n",
       "dim_vals           [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
       "n_folds                                                            5\n",
       "data_file                                       indy_20161017_02.mat\n",
       "decoder                                                           lr\n",
       "decoder_args       {'trainlag': 4, 'testlag': 4, 'decoding_window...\n",
       "decoder_obj                         LinearRegression(normalize=True)\n",
       "r2                 [0.4021620195891976, 0.4401734155167138, 0.191...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.80s/it]\n",
      "1it [00:44, 44.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.28s/it]\n",
      "2it [01:43, 52.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n",
      "3it [01:58, 35.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.95s/it]\n",
      "4it [03:15, 52.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.40s/it]\n",
      "5it [03:44, 43.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "6it [03:56, 32.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.32s/it]\n",
      "7it [04:07, 25.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.33s/it]\n",
      "8it [04:21, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.80s/it]\n",
      "9it [04:33, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "10it [04:44, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.65s/it]\n",
      "11it [04:59, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n",
      "12it [05:13, 15.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
      "13it [05:31, 16.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.68s/it]\n",
      "14it [05:46, 15.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.75s/it]\n",
      "15it [06:01, 15.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.72s/it]\n",
      "16it [06:14, 14.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n",
      "17it [06:28, 14.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.57s/it]\n",
      "18it [06:43, 14.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.61s/it]\n",
      "19it [06:57, 14.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.29s/it]\n",
      "20it [07:13, 14.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n",
      "21it [07:31, 15.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.08s/it]\n",
      "22it [07:44, 14.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.14s/it]\n",
      "23it [07:58, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.20s/it]\n",
      "24it [08:12, 14.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.59s/it]\n",
      "25it [08:27, 14.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
      "26it [08:44, 15.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.73s/it]\n",
      "27it [09:04, 16.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n",
      "28it [09:24, 20.16s/it]\n"
     ]
    }
   ],
   "source": [
    "sabes_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "#data_path = '/mnt/sdb1/nc_data/sabes'\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        sur2pos = []\n",
    "        sur2vel = []\n",
    "        sur2enc = []\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "            sur2pos.append(r2_pos)\n",
    "            sur2vel.append(r2_vel)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            sur2enc.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(sur2pos))\n",
    "        su_r2_vel.append(np.array(sur2vel))\n",
    "        su_r2_enc.append(np.array(sur2enc))\n",
    "        \n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    sabes_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_su_df = pd.DataFrame(sabes_su_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/mnt/sdb1/nc_data/sabes_su_df.dat', 'wb') as f:\n",
    "#     f.write(pickle.dumps(sabes_su_l))\n",
    "with open('/home/akumar/nse/neural_control/data/sabes_su_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_su_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment with orientation model encoding r^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.61s/it]\n",
      "1it [00:38, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.38s/it]\n",
      "2it [01:32, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.43s/it]\n",
      "3it [02:38, 55.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.44s/it]\n",
      "4it [03:05, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.97s/it]\n",
      "5it [04:01, 48.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.29s/it]\n",
      "6it [04:43, 46.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.85s/it]\n",
      "7it [05:30, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.72s/it]\n",
      "8it [06:13, 45.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.64s/it]\n",
      "9it [06:40, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.42s/it]\n",
      "10it [07:31, 45.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loco\n",
    "loco_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "# Copied from the submit file\n",
    "loader_params = {'bin_width':50, 'filter_fn':'none', 'filter_kwargs':{}, 'boxcox':0.5, 'spike_threshold':100, 'region': 'S1'}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(loco_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes(data_file, bin_width=loader_params[\"bin_width\"],\n",
    "                     filter_fn=loader_params['filter_fn'], filter_kwargs=loader_params['filter_kwargs'],\n",
    "                     boxcox=loader_params['boxcox'], spike_threshold=loader_params['spike_threshold'], region='S1')\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        sur2pos = []\n",
    "        sur2vel = []\n",
    "        sur2enc = []\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "            sur2pos.append(r2_pos)\n",
    "            sur2vel.append(r2_vel)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            sur2enc.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(sur2pos))\n",
    "        su_r2_vel.append(np.array(sur2vel))\n",
    "        su_r2_enc.append(np.array(sur2enc))\n",
    "        \n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(loco_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method, region='S1')\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    loco_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:13,  9.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Peanut\n",
    "peanut_su_l = []\n",
    "decoder_params = {'trainlag': 0, 'testlag': 0, 'decoding_window': 6}\n",
    "\n",
    "fpath = '/mnt/Secondary/data/peanut/data_dict_peanut_day14.obj'\n",
    "epochs = np.unique(peanut_df['epoch'].values)\n",
    "\n",
    "for i, epoch in tqdm(enumerate(epochs)):    \n",
    "    dat = load_peanut(fpath, epoch, speed_threshold=peanut_df.iloc[0]['speed_threshold'], \n",
    "                      bin_width=peanut_df.iloc[0]['bin_width'], filter_fn='none', filter_kwargs={},\n",
    "                      spike_threshold=peanut_df.iloc[0]['spike_threshold'], boxcox=peanut_df.iloc[0]['boxcox'])\n",
    "\n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_encoding = [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(peanut_df, epoch=epoch, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "                proj_dw[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_.T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw = np.mean(proj_dw, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('epoch', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    peanut_su_l.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Unit Calcs with Trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires trialization dimreduc and a search for best decoding parameters in the segmented setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {'indy_20160426_01': 0,\n",
    "               'indy_20160622_01':1700,\n",
    "               'indy_20160624_03': 500,\n",
    "               'indy_20160627_01': 0,\n",
    "               'indy_20160630_01': 0,\n",
    "               'indy_20160915_01': 0,\n",
    "               'indy_20160921_01': 0,\n",
    "               'indy_20160930_02': 0,\n",
    "               'indy_20160930_05': 300,\n",
    "               'indy_20161005_06': 0,\n",
    "               'indy_20161006_02': 350,\n",
    "               'indy_20161007_02': 950,\n",
    "               'indy_20161011_03': 0,\n",
    "               'indy_20161013_03': 0,\n",
    "               'indy_20161014_04': 0,\n",
    "               'indy_20161017_02': 0,\n",
    "               'indy_20161024_03': 0,\n",
    "               'indy_20161025_04': 0,\n",
    "               'indy_20161026_03': 0,\n",
    "               'indy_20161027_03': 500,\n",
    "               'indy_20161206_02': 5500,\n",
    "               'indy_20161207_02': 0,\n",
    "               'indy_20161212_02': 0,\n",
    "               'indy_20161220_02': 0,\n",
    "               'indy_20170123_02': 0,\n",
    "               'indy_20170124_01': 0,\n",
    "               'indy_20170127_03': 0,\n",
    "               'indy_20170131_02': 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import reach_segment_sabes\n",
    "from loaders import segment_peanut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.78s/it]\n",
      "/home/akumar/anaconda3/envs/dyn/lib/python3.7/site-packages/ipykernel_launcher.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_21463/3381761480.py\u001b[0m(28)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     26 \u001b[0;31m                  for idx in valid_transitions])\n",
      "\u001b[0m\u001b[0;32m     27 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 28 \u001b[0;31m    \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     29 \u001b[0;31m    \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_file'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     30 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "(35240, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [01:34, ?it/s]\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21463/3381761480.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   for idx in valid_transitions])\n\u001b[1;32m     27\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_file'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21463/3381761480.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                   for idx in valid_transitions])\n\u001b[1;32m     27\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_file'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dyn/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dyn/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Same sort of calculation but now trialized\n",
    "su_trialized_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'], segment=False)\n",
    "    \n",
    "    dat_segment = reach_segment_sabes(dat, start_time=start_times[data_file.split('.mat')[0]])\n",
    "\n",
    "    X = dat['spike_rates']\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    # Exclude exceedingly short transitions\n",
    "    T = 11\n",
    "    t = np.array([t_[1] - t_[0] for t_ in dat_segment['transition_times']])\n",
    "    valid_transitions = np.arange(t.size)[t >= T]\n",
    "\n",
    "    X = np.array([dat['spike_rates'][0, dat_segment['transition_times'][idx][0]:dat_segment['transition_times'][idx][1]] \n",
    "                  for idx in valid_transitions])\n",
    "    Z = np.array([dat['behavior'][dat_segment['transition_times'][idx][0]:dat_segment['transition_times'][idx][1]] \n",
    "                  for idx in valid_transitions])\n",
    "\n",
    "    r = {}\n",
    "    r['data_file'] = data_file\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X[0].shape[-1]))\n",
    "    su_mmse = np.zeros((5, X[0].shape[-1]))\n",
    "    su_pi = np.zeros((5, X[0].shape[-1]))\n",
    "    su_fcca = np.zeros((5, X[0].shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X[0].shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X[0].shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X[0].shape[-1]))\n",
    "    \n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "\n",
    "        ztrain = list(Z[train_idxs])\n",
    "        ztest = list(Z[test_idxs])\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = list(X[train_idxs])\n",
    "        xtest = list(X[test_idxs])\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X[0].shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = list(X[train_idxs])\n",
    "            xtest = list(X[test_idxs])\n",
    "\n",
    "            xtrain = [x[:, neu_idx][:, np.newaxis] for x in xtrain]\n",
    "            xtest = [x[:, neu_idx][:, np.newaxis] for x in xtest]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_vel.append(np.array(r2_vel))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X[0].shape[-1]):\n",
    "\n",
    "            xtrain = list(X[train_idxs])\n",
    "            xtest = list(X[test_idxs])\n",
    "\n",
    "            xtrain = np.vstack([x[:, neu_idx][:, np.newaxis] for x in xtrain])\n",
    "            xtest = np.vstack([x[:, neu_idx][:, np.newaxis] for x in xtest])\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = list(X[train_idxs])\n",
    "        xtest = list(X[test_idxs])\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = [x @ V for x in xtrain] \n",
    "                xtest_proj = [x @ V for x in xtest]\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    su_trialized_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c52494c424e88c3f855a8aeb34b231af4706f7aa247f66fb47c890a5ab8814ab"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
