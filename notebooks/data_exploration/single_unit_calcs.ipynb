{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import time\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/akumar/nse/neural_control')\n",
    "from utils import apply_df_filters, calc_loadings, calc_cascaded_loadings\n",
    "from loaders import load_sabes, load_peanut, load_cv\n",
    "from decoders import lr_decoder, lr_encoder\n",
    "from subspaces import SubspaceIdentification, IteratedStableEstimator, estimate_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dca.cov_util import calc_cross_cov_mats_from_data, calc_pi_from_cross_cov_mats\n",
    "from dca_research.kca import calc_mmse_from_cross_cov_mats\n",
    "from dca_research.lqg import build_loss as build_lqg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/sdb1/nc_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/sabes_decoding_df.dat' % data_path, 'rb') as f:\n",
    "    sabes_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/peanut_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    peanut_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/cv_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    cv_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/loco_dimreduc_df.dat' % data_path, 'rb') as f:\n",
    "    loco_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single unit calculations - no trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim                                                               10\n",
       "fold_idx                                                           0\n",
       "train_idxs         [1983, 1984, 1985, 1986, 1987, 1988, 1989, 199...\n",
       "test_idxs          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "dimreduc_method                                                  KCA\n",
       "dimreduc_args        {'T': 3, 'causal_weights': (1, 1), 'n_init': 5}\n",
       "coef               [[0.04434436239872305, -0.1047764350198416, -0...\n",
       "score                          tensor(141.1882, dtype=torch.float64)\n",
       "bin_width                                                         50\n",
       "filter_fn                                                       none\n",
       "filter_kwargs                                                     {}\n",
       "boxcox                                                           0.5\n",
       "spike_threshold                                                  100\n",
       "dim_vals           [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
       "n_folds                                                            5\n",
       "data_file                                       indy_20161017_02.mat\n",
       "decoder                                                           lr\n",
       "decoder_args       {'trainlag': 4, 'testlag': 4, 'decoding_window...\n",
       "decoder_obj                         LinearRegression(normalize=True)\n",
       "r2                 [0.4021620195891976, 0.4401734155167138, 0.191...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:25<00:00, 25.11s/it]\n",
      "1it [01:09, 69.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:34<00:00, 34.96s/it]\n",
      "2it [02:41, 82.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.11s/it]\n",
      "3it [03:09, 57.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:48<00:00, 48.27s/it]\n",
      "4it [05:18, 86.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:20<00:00, 20.25s/it]\n",
      "5it [05:59, 69.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.43s/it]\n",
      "6it [06:18, 52.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.10s/it]\n",
      "7it [06:36, 41.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.70s/it]\n",
      "8it [07:01, 36.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.89s/it]\n",
      "9it [07:22, 31.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.34s/it]\n",
      "10it [07:40, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.12s/it]\n",
      "11it [08:12, 28.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.97s/it]\n",
      "12it [08:37, 27.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.87s/it]\n",
      "13it [09:10, 29.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.49s/it]\n",
      "14it [09:33, 27.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
      "15it [09:57, 26.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.42s/it]\n",
      "16it [10:22, 25.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.77s/it]\n",
      "17it [10:46, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.15s/it]\n",
      "18it [11:12, 25.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
      "19it [11:36, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.40s/it]\n",
      "20it [12:02, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.67s/it]\n",
      "21it [12:42, 29.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.66s/it]\n",
      "22it [13:05, 27.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.97s/it]\n",
      "23it [13:31, 27.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.20s/it]\n",
      "24it [13:55, 26.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.72s/it]\n",
      "25it [14:28, 28.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.55s/it]\n",
      "26it [14:55, 27.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.58s/it]\n",
      "27it [15:31, 30.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.82s/it]\n",
      "28it [16:03, 34.42s/it]\n"
     ]
    }
   ],
   "source": [
    "sabes_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "data_path = '/mnt/sdb1/nc_data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        sur2pos = []\n",
    "        sur2vel = []\n",
    "        sur2enc = []\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "            sur2pos.append(r2_pos)\n",
    "            sur2vel.append(r2_vel)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            sur2enc.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(sur2pos))\n",
    "        su_r2_vel.append(np.array(sur2vel))\n",
    "        su_r2_enc.append(np.array(sur2enc))\n",
    "        \n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    sabes_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sabes_su_df = pd.DataFrame(sabes_su_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/sdb1/nc_data/sabes_su_df.dat', 'wb') as f:\n",
    "    f.write(pickle.dumps(sabes_su_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.61s/it]\n",
      "1it [00:38, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.38s/it]\n",
      "2it [01:32, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.43s/it]\n",
      "3it [02:38, 55.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.44s/it]\n",
      "4it [03:05, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.97s/it]\n",
      "5it [04:01, 48.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.29s/it]\n",
      "6it [04:43, 46.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.85s/it]\n",
      "7it [05:30, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.72s/it]\n",
      "8it [06:13, 45.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.64s/it]\n",
      "9it [06:40, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.42s/it]\n",
      "10it [07:31, 45.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loco\n",
    "loco_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "# Copied from the submit file\n",
    "loader_params = {'bin_width':50, 'filter_fn':'none', 'filter_kwargs':{}, 'boxcox':0.5, 'spike_threshold':100, 'region': 'S1'}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(loco_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes(data_file, bin_width=loader_params[\"bin_width\"],\n",
    "                     filter_fn=loader_params['filter_fn'], filter_kwargs=loader_params['filter_kwargs'],\n",
    "                     boxcox=loader_params['boxcox'], spike_threshold=loader_params['spike_threshold'], region='S1')\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        sur2pos = []\n",
    "        sur2vel = []\n",
    "        sur2enc = []\n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "            sur2pos.append(r2_pos)\n",
    "            sur2vel.append(r2_vel)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            sur2enc.append(r2_encoding)\n",
    "\n",
    "\n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(sur2pos))\n",
    "        su_r2_vel.append(np.array(sur2vel))\n",
    "        su_r2_enc.append(np.array(sur2enc))\n",
    "        \n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(loco_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method, region='S1')\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    loco_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:13,  9.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Peanut\n",
    "peanut_su_l = []\n",
    "decoder_params = {'trainlag': 0, 'testlag': 0, 'decoding_window': 6}\n",
    "\n",
    "fpath = '/mnt/Secondary/data/peanut/data_dict_peanut_day14.obj'\n",
    "epochs = np.unique(peanut_df['epoch'].values)\n",
    "\n",
    "for i, epoch in tqdm(enumerate(epochs)):    \n",
    "    dat = load_peanut(fpath, epoch, speed_threshold=peanut_df.iloc[0]['speed_threshold'], \n",
    "                      bin_width=peanut_df.iloc[0]['bin_width'], filter_fn='none', filter_kwargs={},\n",
    "                      spike_threshold=peanut_df.iloc[0]['spike_threshold'], boxcox=peanut_df.iloc[0]['boxcox'])\n",
    "\n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_encoding = [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(peanut_df, epoch=epoch, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "                proj_dw[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_.T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw = np.mean(proj_dw, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('epoch', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    peanut_su_l.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barycentric plotting\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Unit Calcs with Trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires trialization dimreduc and a search for best decoding parameters in the segmented setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {'indy_20160426_01': 0,\n",
    "               'indy_20160622_01':1700,\n",
    "               'indy_20160624_03': 500,\n",
    "               'indy_20160627_01': 0,\n",
    "               'indy_20160630_01': 0,\n",
    "               'indy_20160915_01': 0,\n",
    "               'indy_20160921_01': 0,\n",
    "               'indy_20160930_02': 0,\n",
    "               'indy_20160930_05': 300,\n",
    "               'indy_20161005_06': 0,\n",
    "               'indy_20161006_02': 350,\n",
    "               'indy_20161007_02': 950,\n",
    "               'indy_20161011_03': 0,\n",
    "               'indy_20161013_03': 0,\n",
    "               'indy_20161014_04': 0,\n",
    "               'indy_20161017_02': 0,\n",
    "               'indy_20161024_03': 0,\n",
    "               'indy_20161025_04': 0,\n",
    "               'indy_20161026_03': 0,\n",
    "               'indy_20161027_03': 500,\n",
    "               'indy_20161206_02': 5500,\n",
    "               'indy_20161207_02': 0,\n",
    "               'indy_20161212_02': 0,\n",
    "               'indy_20161220_02': 0,\n",
    "               'indy_20170123_02': 0,\n",
    "               'indy_20170124_01': 0,\n",
    "               'indy_20170127_03': 0,\n",
    "               'indy_20170131_02': 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import reach_segment_sabes\n",
    "from loaders import segment_peanut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sort of calculation but now trialized\n",
    "_su_trialized_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    dat = reach_segment_sabes(dat, start_time=\n",
    "\n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "    r = {}\n",
    "    r['data_file'] = data_file\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw = np.zeros((5, 4, dims.size, xtrain.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, xtrain.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_vel.append(np.array(r2_vel))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "\n",
    "                proj_dw[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw = np.mean(proj_dw, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    sabes_su_l.append(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c52494c424e88c3f855a8aeb34b231af4706f7aa247f66fb47c890a5ab8814ab"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
