{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "import time\n",
    "import torch\n",
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/akumar/nse/neural_control')\n",
    "from utils import apply_df_filters, calc_loadings, calc_cascaded_loadings\n",
    "from loaders import load_sabes, load_peanut, load_cv\n",
    "from decoders import lr_decoder, lr_encoder\n",
    "from subspaces import SubspaceIdentification, IteratedStableEstimator, estimate_autocorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dca.cov_util import calc_cross_cov_mats_from_data, calc_pi_from_cross_cov_mats\n",
    "from dca_research.kca import calc_mmse_from_cross_cov_mats\n",
    "from dca_research.lqg import build_loss as build_lqg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/sabes_decoding_df.dat', 'rb') as f:\n",
    "    sabes_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/peanut_dimreduc_df.dat', 'rb') as f:\n",
    "    peanut_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/cv_dimreduc_df.dat', 'rb') as f:\n",
    "    cv_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/akumar/nse/neural_control/data/loco_dimreduc_df.dat', 'rb') as f:\n",
    "    loco_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single unit calculations - no trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dim                                                               10\n",
       "fold_idx                                                           0\n",
       "train_idxs         [1983, 1984, 1985, 1986, 1987, 1988, 1989, 199...\n",
       "test_idxs          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...\n",
       "dimreduc_method                                                  KCA\n",
       "dimreduc_args        {'T': 3, 'causal_weights': (1, 1), 'n_init': 5}\n",
       "coef               [[0.04434436239872305, -0.1047764350198416, -0...\n",
       "score                          tensor(141.1882, dtype=torch.float64)\n",
       "bin_width                                                         50\n",
       "filter_fn                                                       none\n",
       "filter_kwargs                                                     {}\n",
       "boxcox                                                           0.5\n",
       "spike_threshold                                                  100\n",
       "dim_vals           [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
       "n_folds                                                            5\n",
       "data_file                                       indy_20161017_02.mat\n",
       "decoder                                                           lr\n",
       "decoder_args       {'trainlag': 4, 'testlag': 4, 'decoding_window...\n",
       "decoder_obj                         LinearRegression(normalize=True)\n",
       "r2                 [0.4021620195891976, 0.4401734155167138, 0.191...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sabes_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.72s/it]\n",
      "1it [00:43, 43.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.65s/it]\n",
      "2it [01:40, 51.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.09s/it]\n",
      "3it [01:54, 34.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:33<00:00, 33.92s/it]\n",
      "4it [03:11, 51.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:15<00:00, 15.06s/it]\n",
      "5it [03:40, 43.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n",
      "6it [03:51, 32.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.68s/it]\n",
      "7it [04:03, 25.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.74s/it]\n",
      "8it [04:16, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.19s/it]\n",
      "9it [04:28, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "10it [04:39, 16.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.09s/it]\n",
      "11it [04:53, 15.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.00s/it]\n",
      "12it [05:07, 15.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n",
      "13it [05:25, 16.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.24s/it]\n",
      "14it [05:40, 15.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.37s/it]\n",
      "15it [05:55, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
      "16it [06:08, 14.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n",
      "17it [06:21, 14.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.15s/it]\n",
      "18it [06:36, 14.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.11s/it]\n",
      "19it [06:49, 14.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n",
      "20it [07:05, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.47s/it]\n",
      "21it [07:23, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.55s/it]\n",
      "22it [07:35, 14.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.72s/it]\n",
      "23it [07:49, 14.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.89s/it]\n",
      "24it [08:03, 14.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:06<00:00,  6.28s/it]\n",
      "25it [08:19, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.90s/it]\n",
      "26it [08:34, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  7.40s/it]\n",
      "27it [08:53, 16.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.28s/it]\n",
      "28it [09:13, 19.76s/it]\n"
     ]
    }
   ],
   "source": [
    "sabes_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_vel.append(np.array(r2_vel))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    sabes_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orientation tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.61s/it]\n",
      "1it [00:38, 39.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:22<00:00, 22.38s/it]\n",
      "2it [01:32, 47.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:29<00:00, 29.43s/it]\n",
      "3it [02:38, 55.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.44s/it]\n",
      "4it [03:05, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.97s/it]\n",
      "5it [04:01, 48.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:17<00:00, 17.29s/it]\n",
      "6it [04:43, 46.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:18<00:00, 18.85s/it]\n",
      "7it [05:30, 46.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:16<00:00, 16.72s/it]\n",
      "8it [06:13, 45.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:10<00:00, 10.64s/it]\n",
      "9it [06:40, 39.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spikes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:21<00:00, 21.42s/it]\n",
      "10it [07:31, 45.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# Loco\n",
    "loco_su_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "# Copied from the submit file\n",
    "loader_params = {'bin_width':50, 'filter_fn':'none', 'filter_kwargs':{}, 'boxcox':0.5, 'spike_threshold':100, 'region': 'S1'}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(loco_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes(data_file, bin_width=loader_params[\"bin_width\"],\n",
    "                     filter_fn=loader_params['filter_fn'], filter_kwargs=loader_params['filter_kwargs'],\n",
    "                     boxcox=loader_params['boxcox'], spike_threshold=loader_params['spike_threshold'], region='S1')\n",
    "    \n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw_pos = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_dw_vel = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_vel.append(np.array(r2_vel))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(loco_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method, region='S1')\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                proj_dw_pos[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[0:2, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_dw_vel[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_[2:4, :].T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw_pos = np.mean(proj_dw_pos, axis=0)\n",
    "    proj_dw_vel = np.mean(proj_dw_vel, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw_pos', 'proj_dw_vel', 'proj_ew', 'decoder_params'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    loco_su_l.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:13,  9.20s/it]\n"
     ]
    }
   ],
   "source": [
    "# Peanut\n",
    "peanut_su_l = []\n",
    "decoder_params = {'trainlag': 0, 'testlag': 0, 'decoding_window': 6}\n",
    "\n",
    "fpath = '/mnt/Secondary/data/peanut/data_dict_peanut_day14.obj'\n",
    "epochs = np.unique(peanut_df['epoch'].values)\n",
    "\n",
    "for i, epoch in tqdm(enumerate(epochs)):    \n",
    "    dat = load_peanut(fpath, epoch, speed_threshold=peanut_df.iloc[0]['speed_threshold'], \n",
    "                      bin_width=peanut_df.iloc[0]['bin_width'], filter_fn='none', filter_kwargs={},\n",
    "                      spike_threshold=peanut_df.iloc[0]['spike_threshold'], boxcox=peanut_df.iloc[0]['boxcox'])\n",
    "\n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, X.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_encoding = [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(peanut_df, epoch=epoch, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params, include_velocity=False, include_acc=False)\n",
    "\n",
    "                proj_dw[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_.T, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw = np.mean(proj_dw, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('epoch', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    peanut_su_l.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Unit Calcs with Trialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This requires trialization dimreduc and a search for best decoding parameters in the segmented setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_times = {'indy_20160426_01': 0,\n",
    "               'indy_20160622_01':1700,\n",
    "               'indy_20160624_03': 500,\n",
    "               'indy_20160627_01': 0,\n",
    "               'indy_20160630_01': 0,\n",
    "               'indy_20160915_01': 0,\n",
    "               'indy_20160921_01': 0,\n",
    "               'indy_20160930_02': 0,\n",
    "               'indy_20160930_05': 300,\n",
    "               'indy_20161005_06': 0,\n",
    "               'indy_20161006_02': 350,\n",
    "               'indy_20161007_02': 950,\n",
    "               'indy_20161011_03': 0,\n",
    "               'indy_20161013_03': 0,\n",
    "               'indy_20161014_04': 0,\n",
    "               'indy_20161017_02': 0,\n",
    "               'indy_20161024_03': 0,\n",
    "               'indy_20161025_04': 0,\n",
    "               'indy_20161026_03': 0,\n",
    "               'indy_20161027_03': 500,\n",
    "               'indy_20161206_02': 5500,\n",
    "               'indy_20161207_02': 0,\n",
    "               'indy_20161212_02': 0,\n",
    "               'indy_20161220_02': 0,\n",
    "               'indy_20170123_02': 0,\n",
    "               'indy_20170124_01': 0,\n",
    "               'indy_20170127_03': 0,\n",
    "               'indy_20170131_02': 0,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation import reach_segment_sabes\n",
    "from loaders import segment_peanut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sort of calculation but now trialized\n",
    "_su_trialized_l = []\n",
    "decoder_params = {'trainlag': 4, 'testlag': 4, 'decoding_window': 3}\n",
    "\n",
    "data_path = '/mnt/Secondary/data/sabes'\n",
    "data_files = np.unique(sabes_df['data_file'].values)\n",
    "\n",
    "for i, data_file in tqdm(enumerate(data_files)):    \n",
    "    dat = load_sabes('%s/%s' % (data_path, data_file), bin_width=sabes_df.iloc[0][\"bin_width\"],\n",
    "                     filter_fn=sabes_df.iloc[0]['filter_fn'], filter_kwargs=sabes_df.iloc[0]['filter_kwargs'],\n",
    "                     boxcox=sabes_df.iloc[0]['boxcox'], spike_threshold=sabes_df.iloc[0]['spike_threshold'])\n",
    "    \n",
    "    dat = reach_segment_sabes(dat, start_time=\n",
    "\n",
    "    X = np.squeeze(dat['spike_rates'])\n",
    "    Z = dat['behavior']\n",
    "    r = {}\n",
    "    r['data_file'] = data_file\n",
    "\n",
    "    kfold = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = []\n",
    "    encoding_weights = []\n",
    "    su_decoding_weights = []\n",
    "    su_encoding_weights = []\n",
    "    su_r2_pos = []\n",
    "    su_r2_vel = []\n",
    "    su_r2_enc = []\n",
    "\n",
    "    # Single unit statistics\n",
    "    su_var = np.zeros((5, X.shape[-1]))\n",
    "    su_mmse = np.zeros((5, X.shape[-1]))\n",
    "    su_pi = np.zeros((5, X.shape[-1]))\n",
    "    su_fcca = np.zeros((5, X.shape[-1]))\n",
    "\n",
    "    # decoding/encoding weights after projection\n",
    "    dims = np.arange(2, 11)\n",
    "    proj_dw = np.zeros((5, 4, dims.size, xtrain.shape[-1]))\n",
    "    proj_ew = np.zeros((5, 4, dims.size, xtrain.shape[-1]))\n",
    "\n",
    "    for fold_idx, (train_idxs, test_idxs) in enumerate(kfold.split(X)):\n",
    "\n",
    "        r = {}\n",
    "\n",
    "        ztrain = Z[train_idxs, :]\n",
    "        ztest = Z[test_idxs, :]\n",
    "\n",
    "        # Population level decoding/encoding - use the coefficient in the linear fit\n",
    "        # Record both the weights in the coefficient but also the loadings onto the SVD\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        ccm = calc_cross_cov_mats_from_data(xtrain, T=10)\n",
    "        ccm = torch.tensor(ccm)\n",
    "\n",
    "        _, _, _, decodingregressor = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "        _, encodingregressor = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "\n",
    "        decoding_weights.append(decodingregressor.coef_)\n",
    "        encoding_weights.append(encodingregressor.coef_)                \n",
    "        \n",
    "        r2_pos_decoding, r2_vel_decoding, r2_encoding = [], [], []\n",
    "        \n",
    "        su_dw = []\n",
    "        su_ew = []            \n",
    "        \n",
    "        for neu_idx in range(X.shape[-1]):           #Fit all neurons one by one\n",
    "            \n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            # Decoding\n",
    "            r2_pos, r2_vel, _, dr = lr_decoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_pos_decoding.append(r2_pos)\n",
    "            r2_vel_decoding.append(r2_vel)\n",
    "            su_dw.append(dr.coef_)\n",
    "\n",
    "            # Encoding\n",
    "            r2_encoding_, er = lr_encoder(xtest, xtrain, ztest, ztrain, **decoder_params)\n",
    "            r2_encoding.append(r2_encoding_)\n",
    "            su_ew.append(er.coef_)        \n",
    "            \n",
    "        su_decoding_weights.append(np.array(su_dw))\n",
    "        su_encoding_weights.append(np.array(su_ew))\n",
    "        \n",
    "        su_r2_pos.append(np.array(r2_pos))\n",
    "        su_r2_vel.append(np.array(r2_vel))\n",
    "        su_r2_enc.append(np.array(r2_encoding))\n",
    "        \n",
    "\n",
    "        lqg_loss = build_lqg_loss(ccm, 1, ortho_lambda=0., project_mmse=False, loss_type='trace')\n",
    "\n",
    "        for neu_idx in range(X.shape[-1]):\n",
    "\n",
    "            xtrain = X[train_idxs, neu_idx][:, np.newaxis]\n",
    "            xtest = X[test_idxs, neu_idx][:, np.newaxis]\n",
    "\n",
    "            su_var[fold_idx, neu_idx] = np.var(xtrain)\n",
    "            proj = np.zeros((ccm.shape[-1], 1))\n",
    "            proj[neu_idx] = 1\n",
    "            proj = torch.tensor(proj)\n",
    "            su_mmse[fold_idx, neu_idx] = calc_mmse_from_cross_cov_mats(ccm[0:4, ...], proj=proj).numpy()\n",
    "            su_pi[fold_idx, neu_idx] = calc_pi_from_cross_cov_mats(torch.unsqueeze(torch.unsqueeze(ccm[0:9, neu_idx ,neu_idx], 1), 2))\n",
    "            su_fcca[fold_idx, neu_idx] = lqg_loss(proj).numpy()            \n",
    "\n",
    "        # Calculate decoding weights based on projection of the data first\n",
    "\n",
    "        xtrain = X[train_idxs, :]\n",
    "        xtest = X[test_idxs, :]\n",
    "\n",
    "        for dr_idx, dimreduc_method in enumerate(['DCA', 'KCA', 'LQGCA', 'PCA']):\n",
    "            for didx, d in enumerate(dims):\n",
    "                df_ = apply_df_filters(sabes_df, data_file=data_file, fold_idx=fold_idx, dim=d, dimreduc_method=dimreduc_method)\n",
    "                if dimreduc_method == 'LQGCA':\n",
    "                    df_ = apply_df_filters(df_, dimreduc_args={'T': 3, 'loss_type': 'trace', 'n_init': 5})\n",
    "                V = df_.iloc[0]['coef']\n",
    "                if dimreduc_method == 'PCA':\n",
    "                    V = V[:, 0:2]\n",
    "\n",
    "                xtrain_proj = xtrain @ V \n",
    "                xtest_proj = xtest @ V\n",
    "\n",
    "                _, _, _, decodingregressor = lr_decoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "                _, encodingregressor = lr_encoder(xtest_proj, xtrain_proj, ztest, ztrain, **decoder_params)\n",
    "\n",
    "                proj_dw[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, decodingregressor.coef_, d2= decoder_params['decoding_window'])\n",
    "                proj_ew[fold_idx, dr_idx, didx, :] = calc_cascaded_loadings(V, encodingregressor.coef_)\n",
    "\n",
    "\n",
    "    # Average results across folds\n",
    "    decoding_weights = np.mean(np.array(decoding_weights), axis=0)\n",
    "    encoding_weights = np.mean(np.array(encoding_weights), axis=0)\n",
    "    su_decoding_weights = np.mean(np.array(su_decoding_weights), axis=0)\n",
    "    su_encoding_weights = np.mean(np.array(su_encoding_weights), axis=0)\n",
    "    \n",
    "    su_r2_pos = np.mean(np.array(su_r2_pos), axis=0)\n",
    "    su_r2_vel = np.mean(np.array(su_r2_vel), axis=0)\n",
    "    su_r2_enc = np.mean(np.array(su_r2_enc), axis=0)\n",
    "\n",
    "    su_var = np.mean(su_var, axis=0)\n",
    "    su_mmse = np.mean(su_mmse, axis=0)\n",
    "    su_pi = np.mean(su_pi, axis=0)\n",
    "    su_fcca = np.mean(su_fcca, axis=0)\n",
    "\n",
    "    proj_dw = np.mean(proj_dw, axis=0)\n",
    "    proj_ew = np.mean(proj_ew, axis=0)\n",
    "\n",
    "    result = {}\n",
    "    for variable in ('data_file', 'decoding_weights', 'encoding_weights', 'su_decoding_weights', 'su_encoding_weights', 'su_r2_pos',\n",
    "                     'su_r2_vel', 'su_r2_enc', 'su_var', 'su_mmse', 'su_pi', 'su_fcca', 'proj_dw', 'proj_ew'):\n",
    "        result[variable] = eval(variable)\n",
    "\n",
    "    sabes_su_l.append(result)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c52494c424e88c3f855a8aeb34b231af4706f7aa247f66fb47c890a5ab8814ab"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('dyn': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
